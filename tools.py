# -*- coding: utf-8 -*-
"""tools.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-9zwClnauSoHSdHFy1z66fHiECA0wDAS
"""

from langchain.agents import tool
import logging
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.schema.messages import HumanMessage, AIMessage
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain.tools import Tool
from langchain_community.tools import (BaseTool, DuckDuckGoSearchResults, WikipediaQueryRun, YouTubeSearchTool, tool)
from openai import OpenAI
import logging
import streamlit as st
from langchain.chat_models import ChatOpenAI
from langchain.agents import tool
import logging
improt os
from dotenv import load_dotenv
load_dotenv()
from helper_functions import *
from vector_db import *

pc = vector_db()


@tool
def agentic_rag_retriever(question: str) :
    """
    Fetch data from vector store in the form of relevant docs and provide results based on the question asked by the user.
    """

    device = "cuda:0" if torch.cuda.is_available() else "cpu"

    llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    api_key=os.getenv("OPENAI_API_KEY"),
    streaming=True,)



    query_vector = [QuerytoEmbedding(question, device).tolist()]
    index = pc.Index("quickstart")

    relevant_docs = index.query(vector = query_vector, top_k=5, include_metadata=True)

    context = build_context(relevant_docs)



    prompt_template = """
    You are an expert in medical field specializing in anaalying medical records and images with detailed diagnosis
    You will be answering questions asked by userd based on the cotext provided in the form of text and images. Use data only from
    from the context provided.:
    {context}
    Question: {question}
    If the anwser cannot be found in the context, just say that you don't know, don't try to make up an answer.
    Don't answer if you are not sure and decline to answer and say "Sorry, I don't have much information about it."
    Just return the helpful ansfrom langchain.schema.messages import HumanMessage, AIMessage, SystemMessagewer in as much as detailed possible.
    Answer:
    """
    prompt=PromptTemplate.from_template(prompt_template)
    qa_chain = LLMChain(llm=llm, prompt=prompt) # Create an LLMChain object


    result = qa_chain.invoke({'context': context, 'question': question})



    query = result['text']


    query_vector = [QuerytoEmbedding(query, device).tolist()]
    relevant_text = index.query(vector = query_vector, top_k=1, filter={
        "type": {"$eq": "text"}}, include_metadata= True)

    relevant_image = relevant_text['matches'][0].metadata['Image']

    return result['text']



@tool
def agentic_rag_image_with_details_retriever(question: str) :
    """
    Fetch data from vector store in the form of relevant docs and provide results based on the question asked by the user. Use this tool specifically when user asks for image or image with relevanty details
    """

    device = "cuda:0" if torch.cuda.is_available() else "cpu"

    llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    api_key='sk-JyKE3KSy0fGOWqefZBzQOHqc-aBs4cdw-8RbWDgIY2T3BlbkFJAQ5HyqK6YCk3vJMrohbIXyAiCWLJbH0pe1iWXBIsYA',
    streaming=True,)



    query_vector = [QuerytoEmbedding(question, device).tolist()]
    index = pc.Index("quickstart")

    relevant_docs = index.query(vector = query_vector, top_k=5, include_metadata=True)

    context = build_context(relevant_docs)



    prompt_template = """
    You are an expert in medical field specializing in anaalying medical records and images with detailed diagnosis
    You will be answering questions asked by userd based on the cotext provided in the form of text and images. Use data only from
    from the context provided. You will focus on questions where user asks for relevant imaages and analysis:
    {context}
    Question: {question}
    If the anwser cannot be found in the context, just say that you don't know, don't try to make up an answer.
    Don't answer if you are not sure and decline to answer and say "Sorry, I don't have much information about it."
    Just return the helpful ansfrom langchain.schema.messages import HumanMessage, AIMessage, SystemMessagewer in as much as detailed possible.
    Answer:
    """
    prompt=PromptTemplate.from_template(prompt_template)
    qa_chain = LLMChain(llm=llm, prompt=prompt) # Create an LLMChain object


    result = qa_chain.invoke({'context': context, 'question': question})



    query = result['text']


    query_vector = [QuerytoEmbedding(query, device).tolist()]
    relevant_text = index.query(vector = query_vector, top_k=1, filter={
        "type": {"$eq": "text"}}, include_metadata= True)

    relevant_image = relevant_text['matches'][0].metadata['Image']

    response = {'text': query, 'image': relevant_image}

    return response